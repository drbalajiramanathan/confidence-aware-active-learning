<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Model Test</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
</head>
<body>
   <h1>TF.js Model Loader</h1>
    <input type="file" id="imageUpload" accept="image/*">
    <br>
    <img id="uploadedImage" alt="Upload preview" style="width: 224px; height: 224px; margin-top: 10px; border: 1px solid #ccc;">
<br>
<button id="predictButton" style="margin-top: 10px; font-size: 16px;">Predict</button>
<p id="result" style="font-weight: bold; font-size: 18px; margin-top: 10px;">Result:</p>
    <p id="status">Loading model...</p>

 <script>
    // --- HTML Elements ---
    const status = document.getElementById('status');
    const imageUpload = document.getElementById('imageUpload');
    const uploadedImage = document.getElementById('uploadedImage');
    const predictButton = document.getElementById('predictButton');
    const resultElement = document.getElementById('result');

    // --- Model ---
    let model;

    // --- 1. Image Preprocessing Function ---
    function preprocessImage(imgElement) {
        // Preprocessing for MobileNetV2: resize to 224x224, 
        // normalize pixels from 0-255 to 0-1.
        return tf.tidy(() => {
            const tensor = tf.browser.fromPixels(imgElement)
                .resizeNearestNeighbor([224, 224])
                .toFloat()
                .div(tf.scalar(255.0)) // Normalize 0-1
                .expandDims();
            return tensor;
        });
    }

    // --- 2. Prediction Function ---
    async function predictImage() {
        if (!model) {
            status.innerText = 'Error: Model is not loaded yet.';
            return;
        }
        if (!uploadedImage.src || uploadedImage.src.endsWith('#')) {
            status.innerText = 'Error: Please upload an image first.';
            return;
        }

        status.innerText = 'Processing and predicting...';

        const tensor = preprocessImage(uploadedImage);

        // Run prediction
        const prediction = await model.predict(tensor);

        // Get the prediction data (probabilities for 1000 classes)
        const data = await prediction.data();

        // Find the index with the highest probability
        const highestProbIndex = data.indexOf(Math.max(...data));
        const confidence = data[highestProbIndex];

        // Display the generic result
        // This model is trained on ImageNet, not HAM10000.
        // It will predict things like 'dog', 'cat', 'car'.
        resultElement.innerText = `Result: ImageNet Class ID ${highestProbIndex}
Confidence: ${(confidence * 100).toFixed(2)}%`;

        status.innerText = 'Prediction complete.';
        
        tensor.dispose();
        prediction.dispose();
    }

    // --- 3. Event Listeners ---
    imageUpload.addEventListener('change', event => {
        const file = event.target.files[0];
        if (file) {
            const reader = new FileReader();
            reader.onload = e => {
                uploadedImage.src = e.target.result;
                uploadedImage.style.display = 'block';
                status.innerText = 'Image loaded. Click "Predict".';
                resultElement.innerText = 'Result:'; // Clear previous result
            };
            reader.readAsDataURL(file);
        }
    });
    
    predictButton.addEventListener('click', predictImage);

    // --- 4. Model Loading Function ---
    async function loadModel() {
        try {
            // This is a standard, pre-hosted MobileNetV2 model
            const modelURL = 'https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v2_100_224/classification/3/default/1';
            
            console.log('Attempting to load model from:', modelURL);
            // Use loadGraphModel for TF Hub models
            model = await tf.loadGraphModel(modelURL, {fromTFHub: true}); 
            
            console.log('Model Loaded Successfully');
            status.innerText = 'MobileNetV2 Loaded. Please upload an image.';
            
        } catch (error) {
            console.error('Error loading model:', error);
            status.innerText = 'Error loading model. See console (F12).';
        }
    }
    
    // Run the model loader on page load
    loadModel();
</script>
</body>
</html>
